# Implementation Plan: Material Takeoff Import Pipeline

**Branch**: `009-sprint-3-material` | **Date**: 2025-10-17 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/home/clachance14/projects/PipeTrak_V2/specs/009-sprint-3-material/spec.md`

## Summary

Implement CSV-based material takeoff import system that processes construction component line items, explodes quantities into discrete trackable components, auto-creates drawings, and validates data with comprehensive error reporting. Upload TAKEOFF - 6031.csv (78 rows) → create ~200 discrete components (Valve, Instrument, Support, Pipe, Fitting, Flange) with identity keys generated from commodity codes, all in <5 seconds with transaction safety.

**Technical Approach**: Supabase Edge Function for server-side CSV processing with PapaParse, drawing normalization via string manipulation utilities, quantity explosion logic in TypeScript, PostgreSQL transactions for atomicity, 3 new progress templates via database migration, ImportPage UI with react-dropzone for drag-and-drop upload.

## Technical Context

**Language/Version**: TypeScript 5.3 (strict mode), React 18
**Primary Dependencies**:
- Frontend: React Router v7, TanStack Query v5, Zod (validation), react-dropzone, react-hook-form
- Backend: Supabase Edge Functions (Deno runtime), PapaParse (CSV parsing)
- Database: Supabase PostgreSQL 15+, PostgREST, Row Level Security (RLS)

**Storage**: Supabase PostgreSQL (components, drawings, progress_templates tables)
**Testing**: Vitest 3 + Testing Library (contract, integration, unit tests)
**Target Platform**: Web (Chrome/Firefox/Safari, modern ES2020+)
**Project Type**: Web SPA (React frontend + Supabase backend)

**Performance Goals**:
- 78-row CSV import in <5s (creates ~200 components)
- 1,000-row CSV import in <60s (creates ~3,000 components)
- Drawing normalization <1ms per drawing
- Validation feedback <200ms

**Constraints**:
- File size limit: 5MB or 10,000 rows (whichever smaller)
- Transaction safety: all-or-nothing imports (rollback on any error)
- RLS enforcement: users can only import into projects they have access to
- CSV format: DRAWING, SPEC, TYPE, DESCRIPTION, SIZE, QTY, CMDTY CODE, Comments (8 columns)

**Scale/Scope**:
- Support 6 component types (3 existing + 3 new)
- Handle up to 10,000 CSV rows per import
- Create 1-100 discrete components per CSV row (based on QTY)
- Real-world test: TAKEOFF - 6031.csv (78 rows → ~200 components)

## Constitution Check
*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### I. Type Safety First
- [x] TypeScript strict mode enabled (verify tsconfig.json) - already configured in project
- [x] No `as` type assertions without justification in Complexity Tracking - CSV parsing uses proper type guards
- [x] Path aliases (`@/*`) used for imports - standard across project
- [x] Database types will be generated from Supabase schema - existing workflow, no new tables (only progress templates)

### II. Component-Driven Development
- [x] New UI components follow shadcn/ui patterns (Radix + Tailwind) - ImportPage uses shadcn dialog, button components
- [x] Components maintain single responsibility - ImportPage (upload), ImportProgress (status), ErrorReportDownload (errors)
- [x] Server state via TanStack Query, client state via Zustand - useImport mutation hook via TanStack Query
- [x] No prop drilling beyond 2 levels - ProjectContext provides project_id, no deep drilling

### III. Testing Discipline
- [x] TDD approach confirmed (tests before implementation) - contract tests → implementation
- [x] Integration tests cover spec acceptance scenarios - 25 acceptance scenarios mapped to tests
- [x] Test files colocated or in tests/ directory - contracts/ for contract tests, tests/integration/ for workflows
- [x] Tests will use Vitest + Testing Library - standard test stack

### IV. Supabase Integration Patterns
- [x] RLS enabled on all new tables - NO new tables (only progress template rows), existing RLS policies apply
- [x] Multi-tenant isolation via organization_id in policies - import enforces project.organization_id via RLS
- [x] TanStack Query wraps all Supabase calls - useImport hook wraps Edge Function invocation
- [x] Auth via AuthContext (no direct supabase.auth in components) - ImportPage uses AuthContext for user_id
- [x] Realtime subscriptions cleaned up on unmount (if applicable) - N/A for import (one-time operation)

### V. Specify Workflow Compliance
- [x] Feature has spec.md in specs/###-feature-name/ - spec.md created in specs/009-sprint-3-material/
- [x] This plan.md follows template structure - following template exactly
- [x] Tasks.md will be generated by /tasks command - deferred to /tasks
- [x] Implementation will follow /implement workflow - will execute via /implement

**Initial Constitution Check**: ✅ PASS (no violations, standard React + Supabase patterns)

## Project Structure

### Documentation (this feature)
```
specs/009-sprint-3-material/
├── spec.md              # Feature specification (complete)
├── plan.md              # This file (/plan command output)
├── research.md          # Phase 0 output (/plan command)
├── data-model.md        # Phase 1 output (/plan command)
├── quickstart.md        # Phase 1 output (/plan command)
├── contracts/           # Phase 1 output (/plan command)
│   ├── import-takeoff.contract.test.tsx
│   ├── quantity-explosion.contract.test.tsx
│   ├── drawing-normalization.contract.test.tsx
│   └── validation.contract.test.tsx
└── tasks.md             # Phase 2 output (/tasks command - NOT created by /plan)
```

### Source Code (repository root)
```
src/
├── components/
│   ├── ImportPage.tsx                    # NEW - Main import UI
│   ├── ImportProgress.tsx                # NEW - Upload progress indicator
│   ├── ErrorReportDownload.tsx           # NEW - Error CSV download button
│   └── ui/                               # Existing shadcn components
│       ├── dialog.tsx
│       ├── button.tsx
│       └── progress.tsx
├── hooks/
│   └── useImport.ts                      # NEW - TanStack Query mutation hook
├── lib/
│   ├── validation.ts                     # Existing validation utilities
│   ├── csv/
│   │   ├── normalize-drawing.ts          # NEW - Drawing normalization logic
│   │   ├── generate-identity-key.ts      # NEW - Identity key generation
│   │   ├── validate-csv.ts               # NEW - CSV validation rules
│   │   └── explode-quantity.ts           # NEW - Quantity explosion logic
│   └── supabase.ts                       # Existing Supabase client
├── pages/
│   └── ImportsPage.tsx                   # EXISTS - Update to show import UI
├── schemas/
│   └── import.ts                         # NEW - Zod schemas for import validation
└── types/
    └── database.types.ts                 # Existing - Auto-generated from Supabase

supabase/
├── functions/
│   └── import-takeoff/                   # NEW - Edge Function for CSV processing
│       ├── index.ts                      # Main handler
│       ├── parser.ts                     # CSV parsing with PapaParse
│       ├── validator.ts                  # Server-side validation
│       └── transaction.ts                # PostgreSQL transaction logic
└── migrations/
    └── 00016_add_pipe_fitting_flange_templates.sql  # NEW - 3 progress templates

tests/
├── contract/
│   └── import/                           # NEW - Contract tests for import logic
│       ├── import-takeoff.contract.test.tsx
│       ├── quantity-explosion.contract.test.tsx
│       ├── drawing-normalization.contract.test.tsx
│       └── validation.contract.test.tsx
└── integration/
    └── 009-material-import/              # NEW - Integration tests
        ├── csv-upload-workflow.test.tsx
        └── error-reporting.test.tsx
```

**Structure Decision**: Standard React SPA structure with Supabase Edge Functions. Frontend components in `src/`, backend Edge Function in `supabase/functions/`. Contract tests validate import logic, integration tests validate end-to-end workflows. Database migration adds 3 progress template rows (Pipe, Fitting, Flange).

## Phase 0: Outline & Research

**Unknowns to Research**:
1. CSV parsing library for Edge Functions (Deno runtime constraints)
2. Drawing normalization algorithm (remove hyphens, leading zeros, uppercase)
3. Identity key generation patterns (zero-padded 3-digit suffixes)
4. PostgreSQL transaction handling in Edge Functions
5. Error report CSV generation and download
6. File upload with progress tracking (react-dropzone integration)

**Research Outputs** (documented in research.md):

### R1: CSV Parsing Library Selection
**Decision**: PapaParse (https://www.papaparse.com/)
**Rationale**:
- Supports header-based parsing (order-independent columns)
- Runs in Deno (ESM-compatible, no Node.js dependencies)
- Handles edge cases (quoted fields, embedded newlines, UTF-8 encoding)
- Mature library (7.4k stars, used in production widely)
- Streaming support for large files (though we limit to 10k rows)

**Alternatives Considered**:
- `csv-parse` (Node.js-specific, requires shims for Deno)
- Manual parsing (error-prone, doesn't handle RFC 4180 edge cases)

### R2: Drawing Normalization Algorithm
**Decision**: String manipulation pipeline
```typescript
function normalizeDrawing(raw: string): string {
  return raw
    .trim()                          // Remove leading/trailing spaces
    .toUpperCase()                   // Convert to uppercase
    .replace(/[-_\s]+/g, '')        // Remove hyphens, underscores, spaces
    .replace(/^0+(?=\d)/g, '');     // Strip leading zeros (keep if only zeros)
}
// "P-001" → "P001", " DRAIN-1 " → "DRAIN1", "p--0-0-1" → "P1"
```
**Rationale**: Simple, deterministic, handles all spec examples, <1ms performance

**Alternatives Considered**:
- Regex-only solution (less readable, harder to debug)
- Third-party library (overkill for simple transformation)

### R3: Identity Key Generation
**Decision**: Template literal with `String.padStart()`
```typescript
function generateIdentityKey(cmdtyCode: string, index: number, qty: number): string {
  if (qty === 1) return cmdtyCode; // Instruments: no suffix
  const suffix = String(index).padStart(3, '0'); // Zero-pad to 3 digits
  return `${cmdtyCode}-${suffix}`;
}
// QTY=4 → "CMDTY-001", "CMDTY-002", "CMDTY-003", "CMDTY-004"
```
**Rationale**: Built-in `padStart()` handles zero-padding, supports 1-999 components per row

**Alternatives Considered**:
- Manual zero-padding (reinventing the wheel)
- UUID generation (loses semantic meaning)

### R4: Supabase Edge Function + PostgreSQL Transactions
**Decision**: Use Supabase client in Edge Function with explicit transactions
```typescript
import { createClient } from '@supabase/supabase-js@2'

const supabaseAdmin = createClient(url, serviceRoleKey); // Server-side only

await supabaseAdmin.rpc('begin_transaction');
try {
  // Insert drawings, components
  await supabaseAdmin.rpc('commit_transaction');
} catch (error) {
  await supabaseAdmin.rpc('rollback_transaction');
  throw error;
}
```
**Rationale**: Service role key bypasses RLS for bulk inserts, but we validate RLS before calling Edge Function (user must have project access). Transactions ensure atomicity.

**Alternatives Considered**:
- Client-side import (slower, no transaction support, exposes logic to client)
- SQL stored procedure (less flexible, harder to test)

### R5: Error Report CSV Generation
**Decision**: Generate CSV in Edge Function, return as downloadable blob
```typescript
// Server: Generate CSV string
const errorCsv = 'Row,Column,Reason\n' +
  errors.map(e => `${e.row},${e.column},"${e.reason}"`).join('\n');
return new Response(errorCsv, {
  headers: { 'Content-Type': 'text/csv', 'Content-Disposition': 'attachment; filename="errors.csv"' }
});

// Client: Trigger download
const blob = new Blob([response.data], { type: 'text/csv' });
const url = URL.createObjectURL(blob);
const a = document.createElement('a');
a.href = url;
a.download = 'import-errors.csv';
a.click();
```
**Rationale**: Server generates CSV (ensures consistency), client triggers browser download

**Alternatives Considered**:
- JSON error response (less user-friendly, requires manual CSV generation)
- Store errors in database (unnecessary persistence)

### R6: File Upload with Progress
**Decision**: react-dropzone + TanStack Query mutation with progress callback
```typescript
import { useDropzone } from 'react-dropzone';
import { useMutation } from '@tanstack/react-query';

const { mutate, progress } = useMutation({
  mutationFn: (file) => uploadCsv(file, { onUploadProgress: (p) => setProgress(p) }),
});

const { getRootProps, getInputProps } = useDropzone({
  accept: { 'text/csv': ['.csv'] },
  onDrop: (files) => mutate(files[0]),
});
```
**Rationale**: react-dropzone handles drag-and-drop UX, TanStack Query handles mutation state

**Alternatives Considered**:
- Native file input (no drag-and-drop, poor UX)
- Manual fetch with XMLHttpRequest (reinventing TanStack Query)

---

## Phase 1: Design & Contracts

### Data Model (see data-model.md)

#### Entity 1: Progress Template (3 new rows)
**Table**: `progress_templates` (existing)
**New Rows**:
1. **Pipe** template:
   - component_type: "Pipe"
   - version: "1.0.0"
   - workflow_type: "discrete"
   - milestones: `[{"name": "Receive", "weight": 50, "order": 1}, {"name": "Install", "weight": 50, "order": 2}]`

2. **Fitting** template:
   - component_type: "Fitting"
   - version: "1.0.0"
   - workflow_type: "discrete"
   - milestones: `[{"name": "Receive", "weight": 50, "order": 1}, {"name": "Install", "weight": 50, "order": 2}]`

3. **Flange** template:
   - component_type: "Flange"
   - version: "1.0.0"
   - workflow_type: "discrete"
   - milestones: `[{"name": "Receive", "weight": 50, "order": 1}, {"name": "Install", "weight": 50, "order": 2}]`

**Validation Rules**:
- component_type UNIQUE per project (enforced by DB constraint)
- milestones.weight SUM must equal 100 (validated by trigger from Sprint 1)

#### Entity 2: Component Attributes (JSONB schema)
**Stored in**: `components.attributes` column
**Schema**:
```typescript
{
  spec: string;              // ES-03, EN-14, PU-93 (metadata)
  description: string;       // "Blind Flg B16.5 cl150..."
  size: string;              // "2", "1X1", "1/2"
  cmdty_code: string;        // FBLAG2DFA2351215 (commodity code)
  comments: string;          // "N/A" or empty string
  original_qty: number;      // Original QTY from CSV (audit)
}
```
**Validation**: Zod schema validates all fields are strings (except original_qty: number)

#### Entity 3: Import Error Report
**Not persisted** (ephemeral, returned to client)
**Schema**:
```typescript
interface ImportError {
  row: number;           // 1-indexed CSV row number
  column: string;        // Column name (DRAWING, QTY, TYPE, etc.)
  reason: string;        // Human-readable error message
}

interface ImportResult {
  success: boolean;
  componentsCreated?: number;
  rowsProcessed?: number;
  rowsSkipped?: number;
  errors?: ImportError[];
}
```

### API Contracts (see contracts/)

#### Contract 1: import-takeoff Edge Function
**File**: `contracts/import-takeoff.contract.test.tsx`
**Endpoint**: `POST /functions/v1/import-takeoff`
**Request**:
```typescript
{
  projectId: string;        // UUID of target project
  csvContent: string;       // Raw CSV file content
  userId: string;           // Auth user ID (from JWT)
}
```
**Response (Success)**:
```typescript
{
  success: true,
  componentsCreated: 203,
  rowsProcessed: 78,
  rowsSkipped: 0
}
```
**Response (Validation Error)**:
```typescript
{
  success: false,
  errors: [
    { row: 15, column: "QTY", reason: "Invalid data type (expected number)" },
    { row: 23, column: "DRAWING", reason: "Required field missing" }
  ]
}
```
**Contract Test**:
```typescript
it('rejects CSV with missing required columns', async () => {
  const invalidCsv = 'DRAWING,SPEC\nP-001,ES-03'; // Missing TYPE, QTY, CMDTY CODE
  const response = await importTakeoff({ projectId, csvContent: invalidCsv, userId });

  expect(response.success).toBe(false);
  expect(response.errors).toContainEqual({
    row: 0,
    column: 'TYPE',
    reason: 'Missing required column: TYPE'
  });
});
```

#### Contract 2: Quantity Explosion Logic
**File**: `contracts/quantity-explosion.contract.test.tsx`
**Function**: `explodeQuantity(row: CsvRow): Component[]`
**Contract Tests**:
```typescript
it('creates 4 discrete components for QTY=4', () => {
  const row = { TYPE: 'Valve', QTY: 4, 'CMDTY CODE': 'VBALU-001' };
  const components = explodeQuantity(row);

  expect(components).toHaveLength(4);
  expect(components[0].identity_key).toBe('VBALU-001-001');
  expect(components[1].identity_key).toBe('VBALU-001-002');
  expect(components[2].identity_key).toBe('VBALU-001-003');
  expect(components[3].identity_key).toBe('VBALU-001-004');
});

it('uses CMDTY CODE directly for instruments (no suffix)', () => {
  const row = { TYPE: 'Instrument', QTY: 1, 'CMDTY CODE': 'ME-55402' };
  const components = explodeQuantity(row);

  expect(components).toHaveLength(1);
  expect(components[0].identity_key).toBe('ME-55402');
});
```

#### Contract 3: Drawing Normalization
**File**: `contracts/drawing-normalization.contract.test.tsx`
**Function**: `normalizeDrawing(raw: string): string`
**Contract Tests**:
```typescript
it('normalizes "P-001" to "P001"', () => {
  expect(normalizeDrawing('P-001')).toBe('P001');
});

it('normalizes " DRAIN-1 " to "DRAIN1"', () => {
  expect(normalizeDrawing(' DRAIN-1 ')).toBe('DRAIN1');
});

it('normalizes "p--0-0-1" to "P1"', () => {
  expect(normalizeDrawing('p--0-0-1')).toBe('P1');
});
```

#### Contract 4: CSV Validation
**File**: `contracts/validation.contract.test.tsx`
**Function**: `validateCsv(csvContent: string): ValidationResult`
**Contract Tests**:
```typescript
it('validates required columns exist', () => {
  const csv = 'DRAWING,SPEC,TYPE'; // Missing QTY, CMDTY CODE
  const result = validateCsv(csv);

  expect(result.valid).toBe(false);
  expect(result.errors).toContainEqual({
    row: 0,
    column: 'QTY',
    reason: 'Missing required column: QTY'
  });
});

it('validates QTY is numeric', () => {
  const csv = 'DRAWING,TYPE,QTY,CMDTY CODE\nP-001,Valve,ABC,V001';
  const result = validateCsv(csv);

  expect(result.valid).toBe(false);
  expect(result.errors).toContainEqual({
    row: 2,
    column: 'QTY',
    reason: 'Invalid data type (expected number)'
  });
});
```

### Quickstart Verification (see quickstart.md)

**Test Data**: TAKEOFF - 6031.csv (78 rows, real project data)

**Verification Steps**:
1. Start local Supabase: `supabase start`
2. Run migrations: `supabase db reset`
3. Create test user + project in UI
4. Navigate to `/imports` page
5. Drag-and-drop TAKEOFF - 6031.csv
6. Verify upload progress shows (0% → 100%)
7. Verify success message: "Successfully imported ~203 components from 78 rows"
8. Navigate to `/components` page
9. Verify components visible with correct identity keys (e.g., VBALU-SECBFLR00M-006-001)
10. Verify drawings auto-created (DRAIN1, P55501, PW55401, etc.)
11. Verify progress templates assigned (Pipe, Fitting, Flange for new types)
12. Test error reporting: Upload invalid CSV (missing QTY column)
13. Verify error report downloads with "Missing required column: QTY"
14. Verify transaction rollback: Check zero components created after error

**Expected Results**:
- 203 components created (78 rows exploded based on QTY)
- 12 unique drawings auto-created
- 3 new progress templates available (Pipe, Fitting, Flange)
- Import time <5 seconds
- Error CSV downloadable with correct format

### Agent Context Update

Update CLAUDE.md with new import context (executed via script in Phase 1).

---

## Phase 2: Task Planning Approach
*This section describes what the /tasks command will do - DO NOT execute during /plan*

**Task Generation Strategy**:
1. Load `.specify/templates/tasks-template.md` as base
2. Generate tasks from Phase 1 artifacts:
   - Each contract test → "Write contract test" task
   - Each entity → "Create migration" or "Define schema" task
   - Each component → "Implement component" task
3. Order tasks by TDD dependencies:
   - Phase 2.1: Write all contract tests (MUST FAIL)
   - Phase 2.2: Implement utilities (drawing normalization, identity keys, explosion logic)
   - Phase 2.3: Implement Edge Function (CSV parsing, validation, transaction handling)
   - Phase 2.4: Implement UI components (ImportPage, progress, error download)
   - Phase 2.5: Wire up integration (useImport hook, route updates)

**Ordering Strategy**:
- **TDD order**: All tests written before implementation (Red → Green)
- **Dependency order**:
  - Migration first (progress templates needed before import)
  - Utilities before Edge Function (normalization, explosion used by Edge Function)
  - Edge Function before UI (UI calls Edge Function)
  - Contract tests before integration tests
- **Parallel execution**: Mark [P] for independent tasks:
  - Contract tests can run in parallel
  - Utility functions can be implemented in parallel
  - UI components can be developed in parallel

**Estimated Output**: 35-40 numbered tasks in tasks.md
- 5 tasks: Database migration + schema validation
- 12 tasks: Contract tests (4 contracts × 3 tests each)
- 8 tasks: Utility implementations (normalization, explosion, validation, identity keys)
- 6 tasks: Edge Function implementation (parser, validator, transaction handler)
- 8 tasks: UI components (ImportPage, progress, error download, routing)
- 5 tasks: Integration tests (CSV upload workflow, error reporting, real data test)

**IMPORTANT**: This phase is executed by the /tasks command, NOT by /plan

---

## Phase 3+: Future Implementation
*These phases are beyond the scope of the /plan command*

**Phase 3**: Task execution (/tasks command creates tasks.md with 35-40 ordered tasks)
**Phase 4**: Implementation (execute tasks.md following TDD workflow: Red → Green → Refactor)
**Phase 5**: Validation (run all tests, execute quickstart.md with TAKEOFF - 6031.csv, measure performance)

**Success Criteria** (from spec.md):
1. CSV Upload: Drag-and-drop TAKEOFF - 6031.csv successfully uploads
2. Quantity Explosion: QTY=4 creates 4 components with sequential keys
3. Component Types: All 6 types import with correct progress templates
4. Drawing Handling: Auto-created drawings normalized correctly
5. Attributes Storage: SPEC, DESCRIPTION, SIZE, CMDTY CODE, Comments in JSONB
6. Validation: Invalid CSV rejected with downloadable error report
7. Performance: 78-row CSV in <5s, 1,000-row CSV in <60s
8. Transaction Safety: Failed imports rollback completely
9. Templates: 3 new progress templates created
10. Test Coverage: ≥70% overall, ≥80% for import logic
11. Real Data Test: TAKEOFF - 6031.csv imports successfully creating ~200 components

---

## Complexity Tracking
*No constitutional violations - all patterns follow standard React + Supabase practices*

(Empty - no complexity deviations)

---

## Progress Tracking

**Phase Status**:
- [x] Phase 0: Research complete (/plan command) - research.md created
- [x] Phase 1: Design complete (/plan command) - data-model.md, contracts/, quickstart.md created
- [x] Phase 2: Task planning complete (/plan command - describe approach only) - Approach documented above
- [ ] Phase 3: Tasks generated (/tasks command)
- [ ] Phase 4: Implementation complete
- [ ] Phase 5: Validation passed

**Gate Status**:
- [x] Initial Constitution Check: PASS (no violations)
- [x] Post-Design Constitution Check: PASS (no new violations after design)
- [x] All NEEDS CLARIFICATION resolved (none existed, all clarified during spec phase)
- [x] Complexity deviations documented (none)

---
*Based on Constitution v1.0.0 - See `.specify/memory/constitution.md`*
